{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 02 : Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propogation of a Perceptron\n",
    "\n",
    "Task 1: Implement the forward propogation of a single perceptron using only numpy and python functions.\n",
    "\n",
    "\n",
    "A single perceptron can be used for binary classification. Perceptron can be used to implement only linearly seprable functions.\n",
    "\n",
    "The perceptron would look like something as follows:\n",
    "\n",
    "![image.png](img/1.png)\n",
    "\n",
    "Source: https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically the Perceptron equation looks as follows :-\n",
    "\n",
    "\n",
    "![image.png](img/math.png)\n",
    "Source : https://towardsdatascience.com/perceptron-the-artificial-neuron-4d8c70d5cc8d\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Where W1 , W1.. Wn are the weights and X1 , X2 ,...Xn are the inputs. Furthere their is a non-linear activation function f(a) that gives output out.\n",
    "\n",
    "Here , W0 is the bias which in the equation can be taken as b.\n",
    "\n",
    "a = x1*w1 + x2*w2 + x3*w3 + b   # Weighted Sum\n",
    "z = f(a)                        # Activation Function\n",
    "\n",
    "In our case the the activation is ReLu\n",
    "\n",
    "The non-linear activation function is the ReLU function which is defined as follows:\n",
    "\n",
    "\n",
    "            x, if x > 0\n",
    "ReLU(x) = \n",
    "            \n",
    "            0, otherwise\n",
    "             \n",
    "Complete the Perceptron Class given below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(x):\n",
    "    \"\"\" Implements the Step function. \"\"\"\n",
    "    pass\n",
    "\n",
    "class Neuron:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" Class that depicts a single neuron. \n",
    "            Input dimension of this neuron is 3. \"\"\"\n",
    "\n",
    "        self.bias = -0.2\n",
    "        self.weights = [-0.5, 0.1, 0.7]\n",
    "        self.input_dim = 3\n",
    "    \n",
    "    def propagate(self, X):\n",
    "        \"\"\" For each instance, calculate a and then z.\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            X: list of lists\n",
    "                The input instances. \"\"\"\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def _weighted_sum(self, inputs):\n",
    "        \"\"\" Calculate the weighted sum from one instance.\n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            inputs: list of numbers\n",
    "                e.g. [0.1, 0.2, 0.7]. \"\"\"\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def _activation_function(self, x):\n",
    "        \"\"\" Returns step(x). \"\"\"\n",
    "        return step(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [[0.1, 0.2, 0.7], [0.5, 0.9, 0.1], [0.02, 0.1, 0.7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = Neuron()\n",
    "outputs = neuron.propagate(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perceptron is made up of just one neuron. So we can make a wrapper class over the neuron to model a Perceptron. The following part is just for demonstrating this. Don't change anything.\n",
    "\n",
    "This demonstrates one forward pass of a perceptron. The next step is to calculate the losses using the errors and change weights by backpropagation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.neuron = Neuron()\n",
    "        \n",
    "    def train(self, X_train, y_train):    \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "        self.errors = []\n",
    "        self.activations = self.forward_propagate()\n",
    "        self.errors = self.calculate_errors()\n",
    "        \n",
    "    def forward_propagate(self):\n",
    "        return self.neuron.propagate(self.X_train)\n",
    "    \n",
    "    def calculate_errors(self):\n",
    "        errors = [self.activations[i] - self.y_train[i] for i in range(len(self.y_train))]\n",
    "        return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = Perceptron()\n",
    "perceptron.train(inputs, [0.3, 0.01, 0.33])\n",
    "perceptron.errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for plotting graphs\n",
    "\n",
    "\n",
    "These are just helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_loss(epoch_his, train_loss_his, test_loss_his):\n",
    "    train_line, = plt.plot(epoch_his,train_loss_his,label = 'train')\n",
    "    test_line, = plt.plot(epoch_his,test_loss_his,label = 'test')\n",
    "    plt.xlabel('EPOCHS')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend([train_line, test_line] , ['train','test'])\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy(epoch_his, train_acc_his, test_acc_his):\n",
    "    train_line, = plt.plot(epoch_his,train_acc_his,label = 'train')\n",
    "    test_line, = plt.plot(epoch_his,test_acc_his,label = 'test')\n",
    "    plt.xlabel('EPOCHS')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend([train_line, test_line] , ['train','test'])\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function (Binary Cross Entropy Loss)\n",
    "\n",
    "\n",
    "\n",
    "The formula for the same is :\n",
    "\n",
    "\n",
    "\n",
    "![image.png](img/cross.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def binary_cross_entropy(pred , Y):\n",
    "    \"\"\"\n",
    "    The binary cross entropy loss.\n",
    "    Note\n",
    "    -pred should be in range [0,1]\n",
    "    -Y should be a binary vector\n",
    "    Inputs\n",
    "    -pred: a numpy array reprsenting predictions made by the model\n",
    "    -Y : a numpy  array representing the target variable\n",
    "    Note that shape(pred) = shape(Y)\n",
    "    Returns\n",
    "    - loss : a scalar value to represent the loss\n",
    "    - d_pred : a vector of the same shape as pred. It represents the error\n",
    "            to be used for backpropogation(shape(d_pred) = shape(pred))\n",
    "    \"\"\"\n",
    "\n",
    "    # clipping the inputs so there is no overflow\n",
    "    epsilon = 1e-11\n",
    "    pred = np.clip(pred , epsilon, 1 - epsilon)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    COMPLETE THE FUNCTION\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Function (0- 1 accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def accuracy_binary(pred , Y):\n",
    "    \"\"\"\n",
    "    Computes the zero one accuracy for a binary classificatiion(how many prediction match actual value)\n",
    "    Inputs:\n",
    "    -pred: Column vector representing probabilities of Class == 1\n",
    "    -Y: Binary column vector\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    COMPLETE\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification on MNIST Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have implemented our own Perceptron we can use it to classify a real world dataset.\n",
    "\n",
    "We will try to train our model to MNIST dataset.MNIST (\"Modified National Institute of Standards and Technology\") is the de facto “Hello World” dataset of computer vision. \n",
    "\n",
    " \n",
    "The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples.\n",
    "\n",
    "You can download and checkout the various benchmarks at http://yann.lecun.com/exdb/mnist/ for MNIST.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/MnistExamples.png)\n",
    "\n",
    "Example of some images in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But as perceptron only does binary classification so we will do classification between number 0 and 1 only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading MNIST Data\n",
    "\n",
    "\n",
    "Download the data from the following link : https://www.kaggle.com/c/digit-recognizer/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data['label']\n",
    "\n",
    "X = data.drop('label' , axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the dimensions of the data . MNIST has 28 x 28 pixel grayscale images which when flattened(converted to 1D) gives 784 size vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting the data for 0 and 1 only\n",
    "\n",
    "# COMPLETE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets try plotting a image\n",
    "\n",
    "sample = 0 #try changing this number\n",
    "\n",
    "number = X.iloc[sample].values.reshape((28, 28))\n",
    "label = Y.iloc[sample]\n",
    "# Plot\n",
    "plt.title('Label is {label}'.format(label=label))\n",
    "plt.imshow(number, cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know and understand the data lets us try to train our model\n",
    "\n",
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first split the data into training and validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x , test_x, train_y , test_y = train_test_split(X.values , Y.values.reshape(-1,1), test_size = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include Learning into the Perceptron\n",
    "\n",
    "Now we will modify our perceptron class to include perceptron learning algorithm.\n",
    "\n",
    "\n",
    "![image.png](img/learn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(x):\n",
    "    \"\"\" Implements the step function. \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(784 , 1) #decalre an array of shape 784, 1(classication on MNIST DATA)\n",
    "        self.b = 0\n",
    "        \n",
    "    def forward_propagate(self , x):\n",
    "        pass\n",
    "    \n",
    "    def update_weights(self, x , y , y_dash):\n",
    "        \n",
    "        pass\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "    def train(self,\n",
    "            X_train,\n",
    "            Y_train,\n",
    "            X_test,\n",
    "            Y_test,\n",
    "            metric_fn = accuracy_binary,\n",
    "            loss_fn = binary_cross_entropy,\n",
    "            epochs=200,\n",
    "            record_at = 10,\n",
    "            verbose = True):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        This function trains the neural network\n",
    "        Inputs:\n",
    "        - X_train : The training dataset\n",
    "        - Y_train : The training target values\n",
    "        - X_test : The testing dataset\n",
    "        - Y_test : The testing target values\n",
    "        - metric : The metric function for assesing the model (default : accuracy_binary)\n",
    "        - loss_function_string : The loss function (default : mean_square_error)\n",
    "        - epochs : The number of epochs for which the model will be trained (default : 200)\n",
    "        - record_at :  The epoch interval at which the loss and metric will be recorded (default : 100)\n",
    "        - Verbose : Display the statistics, metrics and progress of the model while training (default : True)\n",
    "        \n",
    "        \"\"\"\n",
    "        train_loss_his = []\n",
    "        train_acc_his = []\n",
    "        test_loss_his = []\n",
    "        test_acc_his = []\n",
    "        epoch_his = []\n",
    "        \n",
    "        #iterate over epochs\n",
    "        for i in range(epochs):\n",
    "            \n",
    "            #iterate over each sample\n",
    "            for j in range(len(X_train)):\n",
    "                \n",
    "                \"\"\"\n",
    "                COMPLETE THIS\n",
    "                \n",
    "                propogate forward then update weights\n",
    "                \n",
    "                \n",
    "                \"\"\"\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            if i % record_at == 0:\n",
    "                prediction = self.forward_propagate(X_train)\n",
    "                train_loss = loss_fn(prediction,Y_train)\n",
    "                train_acc = metric_fn(prediction,Y_train)\n",
    "\n",
    "                test_prediction  = self.forward_propagate(X_test)\n",
    "                test_loss= loss_fn(test_prediction,Y_test)\n",
    "                test_acc = metric_fn(test_prediction,Y_test)\n",
    "\n",
    "                train_loss_his.append(train_loss)\n",
    "                train_acc_his.append(train_acc)\n",
    "                test_loss_his.append(test_loss)\n",
    "                test_acc_his.append(test_acc)\n",
    "                epoch_his.append(i)\n",
    "\n",
    "                if verbose:\n",
    "                    print(\"{}th EPOCH:\\nTraining Loss:{}|Training Accuracy:{}|Test Loss:{}|Test Accuracy:{}\".\\\n",
    "                      format(i , train_loss , train_acc,test_loss,test_acc))\n",
    "        train_loss_his = np.array(train_loss_his).reshape(-1)\n",
    "        train_acc_his = np.array(train_acc_his).reshape(-1)\n",
    "        test_loss_his = np.array(test_loss_his).reshape(-1)\n",
    "        test_acc_his = np.array(test_acc_his).reshape(-1)\n",
    "        epoch_his = np.array(epoch_his).reshape(-1)\n",
    "        return train_loss_his,train_acc_his,test_loss_his,test_acc_his,epoch_his\n",
    "     \n",
    "    def metric_function(self,X,Y,metric_fn=accuracy_binary):\n",
    "        \n",
    "        prediction , _ = self.forward(X)\n",
    "        acc = metric_fn(prediction,Y)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Perceptron()\n",
    "\n",
    "train_loss_his,train_acc_his,test_loss_his,test_acc_his,epoch_his=model.train(train_x , train_y, test_x , test_y,\n",
    "            metric_fn = accuracy_binary,\n",
    "            loss_fn=binary_cross_entropy,\n",
    "            epochs=20,\n",
    "            record_at=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(epoch_his,train_loss_his,test_loss_his)\n",
    "plot_accuracy(epoch_his,train_acc_his,test_acc_his)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
